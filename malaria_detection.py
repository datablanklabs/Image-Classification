# -*- coding: utf-8 -*-
"""Malaria_Detection_Full_Code_StevenMoxley.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ANkTVrGTA-poZ-Sk1egZa-09LRo271T1

# **Malaria Detection**

##<b>Problem Definition</b>
**The context:** Why is this problem important to solve?<br>
**The objectives:** What is the intended goal?<br>
**The key questions:** What are the key questions that need to be answered?<br>
**The problem formulation:** What is it that we are trying to solve using data science?

## <b>Data Description </b>

There are a total of 24,958 train and 2,600 test images (colored) that we have taken from microscopic images. These images are of the following categories:<br>


**Parasitized:** The parasitized cells contain the Plasmodium parasite which causes malaria<br>
**Uninfected:** The uninfected cells are free of the Plasmodium parasites<br>

###<b> Mount the Drive
"""

from google.colab import drive # Google Drive
drive.mount("/content/drive")

"""### <b>Loading libraries</b>"""

import os  # Interact with the operating system and filesystem
import pathlib
import PIL
import PIL.Image
import glob
import re
import cv2
import random  # Random number generator
import numpy as np  # Numerical calculations
import pandas as pd  # For loading data from CSV into DataFrames]
import tensorflow as tf  # TensorFlow main module
import keras # main neural network module
from tensorflow.keras.models import Sequential  # Create sequential layered neural networks
from tensorflow.keras.layers import (
   Input,
   Conv2D,  # 2D convolutional layers for images
   LeakyReLU,  # Activation function layers
   MaxPooling2D,  # Maximum pooling layers
   Flatten,  # Flatten layers into 1D vectors
   Dense,  # Fully connected layer
   Dropout,  # Delete/ignore certain neurons to avoid overfitting
   BatchNormalization  # Normalize layer inputs for improved performance
)
from tensorflow.keras.optimizers import ( # Optimizers
    Adam,
    Adafactor,
    Lamb
)
from tensorflow.keras.utils import to_categorical  # One-hot encoder for categorical features
from tensorflow.keras.losses import CategoricalCrossentropy  # Loss function
from tensorflow.keras.metrics import Accuracy  # Accuracy metric to evaluate model performance
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.applications import VGG16
from sklearn.preprocessing import OneHotEncoder  # One-hot encoder for categorical features
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder # encode text category labels
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report
from collections import Counter
import matplotlib.pyplot as plt  # Create charts
import seaborn as sns # Create visualizations

"""### <b>Let us load the data</b>

**Note:**
- You must download the dataset from the link provided on Olympus and upload the same to your Google Drive. Then unzip the folder.
"""

!unzip /content/drive/MyDrive/cell_images.zip

"""The extracted folder has different folders for train and test data will contain the different sizes of images for parasitized and uninfected cells within the respective folder name.

The size of all images must be the same and should be converted to 4D arrays so that they can be used as an input for the convolutional neural network. Also, we need to create the labels for both types of images to be able to train and test the model.

Let's do the same for the training data first and then we will use the same code for the test data as well.
"""

ROOT_DATA_DIR = '/cell_images/train/'

INFECTED_DIR = os.path.join(ROOT_DATA_DIR, 'parasitized')
UNINFECTED_DIR = os.path.join(ROOT_DATA_DIR, 'uninfected')

infected_files = glob.glob(INFECTED_DIR+'/*.png')
uninfected_files = glob.glob(UNINFECTED_DIR+'/*.png')

print(f'Amount of parasitized images: {len(infected_files)}')
print(f'Amount of uninfected images: {len(uninfected_files)}')
print(f'Total Images: {len(infected_files) + len(uninfected_files)}')

train_data_dir = pathlib.Path('/content/cell_images/train/')
test_data_dir = pathlib.Path('/content/cell_images/test/')
train_image_count = len(list(train_data_dir.glob('*/*.png')))
test_image_count = len(list(test_data_dir.glob('*/*.png')))
print('Training set has', train_image_count, 'images and testing set has', test_image_count, 'images.')

batch_size = 32
img_height = 180
img_width = 180

"""###<b> Check the shape of train and test images"""

train_ds = tf.keras.utils.image_dataset_from_directory(
  train_data_dir,
  validation_split=0.2,
  subset="training",
  seed=20250802,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  test_data_dir,
  validation_split=0.2,
  subset="validation",
  seed=20250802,
  image_size=(img_height, img_width),
  batch_size=batch_size)

"""###<b> Check the shape of train and test labels"""

train_class_names = train_ds.class_names
val_class_names = val_ds.class_names
print('The classes in the training set are:', train_class_names)
print('The classes in the validation set are:', val_class_names)

"""####<b> Observations and insights:</b>
- The training and validation sets have the same classes as expected.

\### <b>Check the minimum and maximum range of pixel values for train and test images
"""

min_pixel_value = float('inf')
max_pixel_value = float('-inf')

for images, _ in train_ds:
  # Convert tensor to numpy array
  image_array = images.numpy()

  # Find min and max pixel values in the current batch
  batch_min = np.min(image_array)
  batch_max = np.max(image_array)

  # Update overall min and max
  min_pixel_value = min(min_pixel_value, batch_min)
  max_pixel_value = max(max_pixel_value, batch_max)

print(f"Minimum pixel value in training data: {min_pixel_value}")
print(f"Maximum pixel value in training data: {max_pixel_value}")

"""####<b> Observations and insights: </b>
- The pixel values span the entire range from lowest possible value to highest possible value.

###<b> Count the number of values in both uninfected and parasitized
"""

# Count the number of values in both uninfected and parasitized
parasitized_count = 0
uninfected_count = 0

for _, labels in train_ds:
  for label in labels:
    if train_class_names[label] == 'parasitized':
      parasitized_count += 1
    else:
      uninfected_count += 1

print(f"Number of parasitized images in training set: {parasitized_count}")
print(f"Number of uninfected images in training set: {uninfected_count}")

"""###<b>Normalize the images"""

normalization_layer = tf.keras.layers.Rescaling(1./255)

normalized_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), tf.one_hot(y, depth=len(train_class_names)))) # Apply one-hot encoding
normalized_val_ds = val_ds.map(lambda x, y: (normalization_layer(x), tf.one_hot(y, depth=len(val_class_names)))) # Apply one-hot encoding

"""####<b> Observations and insights:</b>
- The number of parasitized and unifected examples is roughly the same.
- As with most neural networks, the images should be normalized.

###<b> Plot to check if the data is balanced
"""

# Data for plotting
class_counts = {'Parasitized': parasitized_count, 'Uninfected': uninfected_count}
classes = list(class_counts.keys())
counts = list(class_counts.values())

plt.figure(figsize=(6, 4))
sns.barplot(x=classes, y=counts)
plt.title('Distribution of Parasitized and Uninfected Images in Training Set')
plt.ylabel('Number of Images')
plt.show()

"""####Observations and insights:###
- The data are roughly balanced

### <b>Data Exploration</b>
Let's visualize the images from the train data

###<b> Visualize the images with subplot(6, 6) and figsize = (12, 12)
"""

plt.figure(figsize=(12, 12))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(6, 6, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(train_class_names[labels[i]])
    plt.axis("off")

for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

"""####<b>Observations and insights:

###<b> Plotting the mean images for parasitized and uninfected

<b> Mean image for parasitized
"""

# Calculate the mean image for parasitized
parasitized_images = []
for images, labels in train_ds:
  for i in range(len(labels)):
    if train_class_names[labels[i]] == 'parasitized':
      parasitized_images.append(images[i].numpy())

mean_parasitized_image = np.mean(parasitized_images, axis=0)
mean_parasitized_image = mean_parasitized_image.astype(np.uint8)

plt.figure(figsize=(6, 6))
plt.imshow(mean_parasitized_image)
plt.title('Mean Image for Parasitized')
plt.axis('off')
plt.show()

"""<b> Mean image for uninfected"""

# Calculate the mean image for uninfected
uninfected_images = []
for images, labels in train_ds:
  for i in range(len(labels)):
    if train_class_names[labels[i]] == 'uninfected':
      uninfected_images.append(images[i].numpy())

mean_uninfected_image = np.mean(uninfected_images, axis=0)
mean_uninfected_image = mean_uninfected_image.astype(np.uint8)

plt.figure(figsize=(6, 6))
plt.imshow(mean_uninfected_image)
plt.title('Mean Image for Uninfected')
plt.axis('off')
plt.show()

"""####<b> Observations and insights:</b>
- The average images are difficult to distinguish.
- Infected cells may be slightly darker on average.

### <b>Converting RGB to HSV of Images using OpenCV

###<b> Converting the train data
"""

hsv_train_images = []
hsv_train_labels = []

for images, labels in train_ds:
  for i in range(images.shape[0]):
    # Convert RGB to HSV
    hsv_image = cv2.cvtColor(images[i].numpy().astype("uint8"), cv2.COLOR_RGB2HSV)
    hsv_train_images.append(hsv_image)
    hsv_train_labels.append(labels[i].numpy())

# Convert lists to numpy arrays
hsv_train_images = np.array(hsv_train_images)
hsv_train_labels = np.array(hsv_train_labels)

print("Shape of HSV training images:", hsv_train_images.shape)
print("Shape of HSV training labels:", hsv_train_labels.shape)

"""###<b> Converting the test data"""

hsv_test_images = []
hsv_test_labels = []

for images, labels in val_ds:
  for i in range(images.shape[0]):
    # Convert RGB to HSV
    hsv_image = cv2.cvtColor(images[i].numpy().astype("uint8"), cv2.COLOR_RGB2HSV)
    hsv_test_images.append(hsv_image)
    hsv_test_labels.append(labels[i].numpy())

# Convert lists to numpy arrays
hsv_test_images = np.array(hsv_test_images)
hsv_test_labels = np.array(hsv_test_labels)

print("Shape of HSV testing images:", hsv_test_images.shape)
print("Shape of HSV testing labels:", hsv_test_labels.shape)

"""####<b>Observations and insights: </b>
- The training and testing images have dimmensions of 180x180 pixels.

###<b> Processing Images using Gaussian Blurring

###<b> Gaussian Blurring on train data
"""

blurred_hsv_train_images = []

for image in hsv_train_images:
  # Apply Gaussian blur (kernel size can be adjusted)
  blurred_image = cv2.GaussianBlur(image, (5, 5), 0)
  blurred_hsv_train_images.append(blurred_image)

# Convert list to numpy array
blurred_hsv_train_images = np.array(blurred_hsv_train_images)

print("Shape of blurred HSV training images:", blurred_hsv_train_images.shape)

"""###<b> Gaussian Blurring on test data"""

blurred_hsv_test_images = []

for image in hsv_test_images:
  # Apply Gaussian blur with the same kernel size as the training data
  blurred_image = cv2.GaussianBlur(image, (5, 5), 0)
  blurred_hsv_test_images.append(blurred_image)

# Convert list to numpy array
blurred_hsv_test_images = np.array(blurred_hsv_test_images)

print("Shape of blurred HSV testing images:", blurred_hsv_test_images.shape)

"""####**Observations and insights: **

**Think About It:** Would blurring help us for this problem statement in any way? What else can we try?
- Some images from the real world may be blurry or off-centered

## **Model Building**

### **Base Model**

####<B>One Hot Encoding the train and test labels
"""

# One-hot encode the training labels
one_hot_train_labels = to_categorical(hsv_train_labels, num_classes=len(train_class_names))

# One-hot encode the testing labels
one_hot_test_labels = to_categorical(hsv_test_labels, num_classes=len(val_class_names))

print("Shape of one-hot encoded training labels:", one_hot_train_labels.shape)
print("Shape of one-hot encoded testing labels:", one_hot_test_labels.shape)

"""###<b> Building the model"""

#Define the base model
base_model = Sequential([
    Input(shape=(img_height, img_width, 3)),
    Conv2D(32, (3, 3), activation='silu'),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='silu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='silu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='silu'),
    Dropout(0.5), # Added dropout layer
    Dense(len(train_class_names), activation='softmax') # Output layer with softmax for classification
])

"""###<b> Compiling the model"""

# Compile the base model
base_model.compile(optimizer='adam',
              loss=CategoricalCrossentropy(),
              metrics=['accuracy'])

base_model.summary()

"""<b> Using Callbacks"""

# Define callbacks
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, verbose=1),
    ModelCheckpoint('best_base_model.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

"""<b> Fit and train our Model"""

# Fit and train the base model
history_base = base_model.fit(
    normalized_train_ds,  # Use the normalized and one-hot encoded training dataset
    validation_data=normalized_val_ds, # Use the normalized and one-hot encoded validation dataset
    epochs=20, # You can adjust the number of epochs
    callbacks=callbacks
)

"""###<b> Evaluating the model on test data"""

# Evaluate Model 1 on the test data
base_loss, base_accuracy = base_model.evaluate(normalized_val_ds)

print(f"Base Model Test Loss: {base_loss}")
print(f"Base Model Test Accuracy: {base_accuracy}")

"""<b> Plotting the confusion matrix"""

# Get predictions on the test data
predictions = base_model.predict(normalized_val_ds)
predicted_labels = np.argmax(predictions, axis=1)

# Get the true labels from the test dataset
true_labels = np.concatenate([y for x, y in normalized_val_ds], axis=0)
true_labels = np.argmax(true_labels, axis=1)

# Generate the confusion matrix
print(classification_report(true_labels, predicted_labels))
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=val_class_names, yticklabels=val_class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""<b>Plotting the train and validation curves"""

# Plot training and validation accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_base.history['accuracy'], label='Training Accuracy')
plt.plot(history_base.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss
plt.subplot(1, 2, 2)
plt.plot(history_base.history['loss'], label='Training Loss')
plt.plot(history_base.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""So now let's try to build another model with few more add on layers and try to check if we can try to improve the model. Therefore try to build a model by adding few layers if required and altering the activation functions.

###<b> Model 1
####<b> Trying to improve the performance of our model by adding new layers

###<b> Building the Model
"""

# Define Model 1 with additional layers
model_1 = Sequential([
    Input(shape=(img_height, img_width, 3)),
    Conv2D(32, (3, 3), activation='silu'),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='silu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='silu'),
    MaxPooling2D((2, 2)),
    Conv2D(256, (3, 3), activation='silu'), # Added layer
    MaxPooling2D((2, 2)), # Added layer
    Flatten(),
    Dense(256, activation='silu'), # Increased dense units
    Dropout(0.5), # Added dropout layer
    Dense(128, activation='silu'), # Added dense layer
    Dropout(0.5), # Added dropout layer
    Dense(len(train_class_names), activation='softmax') # Output layer
])

"""###<b> Compiling the model"""

# Compile Model 1
model_1.compile(optimizer='adam',
              loss=CategoricalCrossentropy(),
              metrics=['accuracy'])

model_1.summary()

"""<b> Using Callbacks"""

# Define callbacks
model1_callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, verbose=1),
    ModelCheckpoint('best_model1.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

"""<b>Fit and Train the model"""

# Fit and train Model 1
history_model_1 = model_1.fit(
    normalized_train_ds,
    validation_data=normalized_val_ds,
    epochs=20,
    callbacks=model1_callbacks
)

"""###<b> Evaluating the model"""

# Evaluate Model 1 on the test data
loss_1, accuracy_1 = model_1.evaluate(normalized_val_ds)

print(f"Model 1 Test Loss: {loss_1}")
print(f"Model 1 Test Accuracy: {accuracy_1}")

"""<b> Plotting the confusion matrix"""

# Get predictions on the test data for Model 1
predictions_model_1 = model_1.predict(normalized_val_ds)
predicted_labels_model_1 = np.argmax(predictions_model_1, axis=1)

# Generate the confusion matrix for Model 1
print(classification_report(true_labels, predicted_labels_model_1))
cm_model_1 = confusion_matrix(true_labels, predicted_labels_model_1)

# Plot the confusion matrix for Model 1
plt.figure(figsize=(8, 6))
sns.heatmap(cm_model_1, annot=True, fmt='d', cmap='Blues', xticklabels=val_class_names, yticklabels=val_class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Model 1')
plt.show()

"""<b> Plotting the train and the validation curves"""

# Plot training and validation accuracy for Model 1
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_model_1.history['accuracy'], label='Training Accuracy')
plt.plot(history_model_1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy (Model 1)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss for Model 1
plt.subplot(1, 2, 2)
plt.plot(history_model_1.history['loss'], label='Training Loss')
plt.plot(history_model_1.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss (Model 1)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""###<b>Think about it:</b><br>
Now let's build a model with LeakyRelu as the activation function  

*  Can the model performance be improved if we change our activation function to LeakyRelu?
*  Can BatchNormalization improve our model?

Let us try to build a model using BatchNormalization and using LeakyRelu as our activation function.

###<b> Model 2 with Batch Normalization

###<b> Building the Model
"""

# Define Model 2 with Batch Normalization and Leaky ReLU
model_2 = Sequential([
    Input(shape=(img_height, img_width, 3)),
    Conv2D(32, (3, 3)),
    BatchNormalization(), # Added Batch Normalization
    LeakyReLU(),          # Using Leaky ReLU activation
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3)),
    BatchNormalization(), # Added Batch Normalization
    LeakyReLU(),          # Using Leaky ReLU activation
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3)),
    BatchNormalization(), # Added Batch Normalization
    LeakyReLU(),          # Using Leaky ReLU activation
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128),
    Dropout(0.5), # Added dropout layer
    BatchNormalization(), # Added Batch Normalization
    LeakyReLU(),          # Using Leaky ReLU activation
    Dense(len(train_class_names), activation='softmax') # Output layer with softmax
])

"""###<b>Compiling the model"""

# Compile Model 2
model_2.compile(optimizer='adam',
              loss=CategoricalCrossentropy(),
              metrics=['accuracy'])

model_2.summary()

"""<b> Using callbacks"""

# Define callbacks
model2_callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, verbose=1),
    ModelCheckpoint('best_model2.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

"""<b>Fit and train the model"""

# Fit and train Model 2
history_model_2 = model_2.fit(
    normalized_train_ds,
    validation_data=normalized_val_ds,
    epochs=20,
    callbacks=model2_callbacks
)

"""####<b>Observations and insights: ____

###<b>Evaluating the model
"""

# Evaluate Model 2 on the test data
loss_2, accuracy_2 = model_2.evaluate(normalized_val_ds)

print(f"Model 2 Test Loss: {loss_2}")
print(f"Model 2 Test Accuracy: {accuracy_2}")

"""<b> Generate the classification report and confusion matrix"""

# Get predictions on the test data for Model 2
predictions_model_2 = model_2.predict(normalized_val_ds)
predicted_labels_model_2 = np.argmax(predictions_model_2, axis=1)

# Generate the confusion matrix for Model 2
print(classification_report(true_labels, predicted_labels_model_2))
cm_model_2 = confusion_matrix(true_labels, predicted_labels_model_2)

# Plot the confusion matrix for Model 2
plt.figure(figsize=(8, 6))
sns.heatmap(cm_model_2, annot=True, fmt='d', cmap='Blues', xticklabels=val_class_names, yticklabels=val_class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Model 2')
plt.show()

"""<b>Plotting the train and validation accuracy"""

# Plot training and validation accuracy for Model 2
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_model_2.history['accuracy'], label='Training Accuracy')
plt.plot(history_model_2.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy (Model 2)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss for Model 1
plt.subplot(1, 2, 2)
plt.plot(history_model_2.history['loss'], label='Training Loss')
plt.plot(history_model_2.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss (Model 2)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""###**Think About It :**<br>

* Can we improve the model with Image Data Augmentation?
* References to image data augmentation can be seen below:
  *   [Image Augmentation for Computer Vision](https://www.mygreatlearning.com/blog/understanding-data-augmentation/)
  *   [How to Configure Image Data Augmentation in Keras?](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)

###<b>Model 3 with Data Augmentation

###<b> Use image data generator
"""

# Get a sample batch from the training dataset
for images, labels in train_ds.take(1):
    sample_images = images.numpy()
    sample_labels = labels.numpy()

# Use ImageDataGenerator to create augmented samples for visualization
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    zoom_range=0.05,
    rotation_range=25,
    width_shift_range=0.05,
    height_shift_range=0.05,
    shear_range=0.05,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Flow from the sample batch for visualization
sample_generator = train_datagen.flow(sample_images, sample_labels, batch_size=1)

"""###**Think About It :**<br>

*  Check if the performance of the model can be improved by changing different parameters in the ImageDataGenerator.

####<B>Visualizing Augmented images
"""

# Generate and display augmented images
plt.figure(figsize=(12, 12))
for i in range(9):
    augmented_batch = next(sample_generator)
    augmented_image = augmented_batch[0][0].astype('uint8') # Get the image from the batch
    augmented_label = np.argmax(augmented_batch[1][0]) # Get the one-hot encoded label and convert to integer

    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(augmented_image)
    plt.title(train_class_names[augmented_label])
    plt.axis("off")

plt.show()

"""####<b>Observations and insights: </b>
- The images are recognizeable but appear zoomed in compared to the originals.

###<b>Building the Model
"""

# Define Model 3 with Batch Normalization and Leaky ReLU
model_3 = Sequential([
    Input(shape=(img_height, img_width, 3)),
    Conv2D(32, (3, 3)),
    BatchNormalization(), # Added Batch Normalization
    LeakyReLU(),          # Using Leaky ReLU activation
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3)),
    BatchNormalization(), # Added Batch Normalization
    LeakyReLU(),          # Using Leaky ReLU activation
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3)),
    BatchNormalization(), # Added Batch Normalization
    LeakyReLU(),          # Using Leaky ReLU activation
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128),
    Dropout(0.5), # Added dropout layer
    BatchNormalization(), # Added Batch Normalization
    LeakyReLU(),          # Using Leaky ReLU activation
    Dense(len(train_class_names), activation='softmax') # Output layer with softmax
])

# Compile Model 3
model_3.compile(optimizer='adam',
              loss=CategoricalCrossentropy(),
              metrics=['accuracy'])

model_3.summary()

"""<b>Using Callbacks"""

# Define callbacks
model3_callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, verbose=1),
    ModelCheckpoint('best_model3.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

"""<b> Fit and Train the model"""

# Fit and train Model 3
history_model_3 = model_3.fit(
    normalized_train_ds,
    validation_data=normalized_val_ds,
    epochs=20,
    callbacks=model3_callbacks
)

"""###<B>Evaluating the model"""

# Evaluate Model 3 on the test data
loss_3, accuracy_3 = model_3.evaluate(normalized_val_ds)

print(f"Model 3 Test Loss: {loss_3}")
print(f"Model 3 Test Accuracy: {accuracy_3}")

"""<B>Plotting the classification report and confusion matrix"""

# Get predictions on the test data for Model 3
predictions_model_3 = model_3.predict(normalized_val_ds)
predicted_labels_model_3 = np.argmax(predictions_model_3, axis=1)

# Generate the confusion matrix for Model 3
print(classification_report(true_labels, predicted_labels_model_3))
cm_model_3 = confusion_matrix(true_labels, predicted_labels_model_3)

# Plot the confusion matrix for Model 3
plt.figure(figsize=(8, 6))
sns.heatmap(cm_model_3, annot=True, fmt='d', cmap='Blues', xticklabels=val_class_names, yticklabels=val_class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Model 3')
plt.show()

"""<b>Plot the train and validation accuracy"""

# Plot training and validation accuracy for Model 3
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_model_3.history['accuracy'], label='Training Accuracy')
plt.plot(history_model_3.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy (Model 3)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss for Model 3
plt.subplot(1, 2, 2)
plt.plot(history_model_3.history['loss'], label='Training Loss')
plt.plot(history_model_3.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss (Model 3)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""<b> Now, let us try to use a pretrained model like VGG16 and check how it performs on our data.

### **Pre-trained model (VGG16)**
- Import VGG16 network upto any layer you choose
- Add Fully Connected Layers on top of it
"""

# Load VGG16 model without the top classification layer
vgg = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))

# Freeze the layers up to layer 20
for layer in vgg.layers[:21]: # Layer 20 is at index 20 (0-based)
    layer.trainable = False

# Create a new sequential model
model_vgg = Sequential()

# Add the layers from VGG16 up to layer 20 (including layer 20)
for layer in vgg.layers[:21]:
    model_vgg.add(layer)

# Add custom classification layers on top
model_vgg.add(Flatten())
model_vgg.add(Dense(256, activation='silu'))
model_vgg.add(Dropout(0.5))
model_vgg.add(Dense(len(train_class_names), activation='softmax'))

model_vgg.summary()

"""###<b>Compiling the model"""

# Compile the VGG16 model
model_vgg.compile(optimizer='adam',
                  loss=CategoricalCrossentropy(),
                  metrics=['accuracy'])

"""<b> using callbacks"""

# Define callbacks
model4_callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, verbose=1),
    ModelCheckpoint('best_model4.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

"""<b>Fit and Train the model"""

# Fit and train Model 4
history_model_4 = model_vgg.fit(
    normalized_train_ds,
    validation_data=normalized_val_ds,
    epochs=20,
    callbacks=model4_callbacks
)

"""<b>Plot the train and validation accuracy"""

# Evaluate Model 4 on the test data
loss_4, accuracy_4 = model_vgg.evaluate(normalized_val_ds)

print(f"Model 4 Test Loss: {loss_4}")
print(f"Model 4 Test Accuracy: {accuracy_4}")

# Plot training and validation accuracy for Model 4
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_model_4.history['accuracy'], label='Training Accuracy')
plt.plot(history_model_4.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy (Model 4)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss for Model 4
plt.subplot(1, 2, 2)
plt.plot(history_model_4.history['loss'], label='Training Loss')
plt.plot(history_model_4.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss (Model 4)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""###**Observations and insights: _____**

*   What can be observed from the validation and train curves?

###<b> Evaluating the model
"""

# Evaluate Model 4 on the test data
loss_4, accuracy_4 = model_vgg.evaluate(normalized_val_ds)

print(f"Model 4 Test Loss: {loss_4}")
print(f"Model 4 Test Accuracy: {accuracy_4}")

"""<b>Plotting the classification report and confusion matrix"""

# Get predictions on the test data for Model 4
predictions_model_4 = model_vgg.predict(normalized_val_ds)
predicted_labels_model_4 = np.argmax(predictions_model_4, axis=1)

# Generate the confusion matrix for Model 4
print(classification_report(true_labels, predicted_labels_model_4))
cm_model_3 = confusion_matrix(true_labels, predicted_labels_model_4)

# Plot the confusion matrix for Model 4
plt.figure(figsize=(8, 6))
sns.heatmap(cm_model_4, annot=True, fmt='d', cmap='Blues', xticklabels=val_class_names, yticklabels=val_class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Model 4')
plt.show()

"""###<b>Think about it:</b>
*  What observations and insights can be drawn from the confusion matrix and classification report?
*  Choose the model with the best accuracy scores from all the above models and save it as a final model.

####<b> Observations and Conclusions drawn from the final model:</b>
- Although the models are roughly comparable in terms of their F1 scores from the  classification reports, the last model (the pre-trained one) outperformed the others.
- Using data augmentation techniques or a pre-trained model were more successful models compared to our initial approaches.

**Improvements that can be done:**<br>


*  Can the model performance be improved using other pre-trained models or different CNN architecture?
*  You can try to build a model using these HSV images and compare them with your other models.

#### **Insights**

####**Refined insights**:
- What are the most meaningful insights from the data relevant to the problem?

####**Comparison of various techniques and their relative performance**:
- How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?

####**Proposal for the final solution design**:
- What model do you propose to be adopted? Why is this the best solution to adopt?
"""